{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start a simple spark session\n",
    "import findspark\n",
    "findspark.init('/home/paul/spark-3.0.0-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('sol').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the walmart stock csv file, have spark infer the data types\n",
    "df = spark.read.csv('Downloads/WMT.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define columns name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema look like\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Date='2019-07-12', Open=114.089996, High=114.769997, Low=113.620003, Close=114.599998, Adj Close=112.534279, Volume=3743300)\n",
      "\n",
      "\n",
      "Row(Date='2019-07-15', Open=114.669998, High=115.080002, Low=114.449997, Close=114.980003, Adj Close=112.907433, Volume=3346000)\n",
      "\n",
      "\n",
      "Row(Date='2019-07-16', Open=115.330002, High=115.489998, Low=114.040001, Close=114.760002, Adj Close=112.691399, Volume=3488200)\n",
      "\n",
      "\n",
      "Row(Date='2019-07-17', Open=114.809998, High=115.169998, Low=114.199997, Close=114.599998, Adj Close=112.534279, Volume=2688500)\n",
      "\n",
      "\n",
      "Row(Date='2019-07-18', Open=114.349998, High=114.779999, Low=113.739998, Close=114.720001, Adj Close=112.652122, Volume=3224800)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print out first 5 columns\n",
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|      Date|              Open|              High|               Low|             Close|         Adj Close|           Volume|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|       252|               252|               252|               252|               252|               252|              252|\n",
      "|   mean|      null|118.08813497222226|119.29162696031744|116.91523788888895|118.12956356349204|117.10961042063497|7487978.968253968|\n",
      "| stddev|      null| 5.058526579992194| 5.007457713071321| 5.029074692710633| 4.966992425100021| 5.322205020724095|4370560.229845061|\n",
      "|    min|2019-07-12|        105.199997|        106.839996|             102.0|        104.050003|        103.137947|          2227400|\n",
      "|    max|2020-07-10|        132.389999|        133.380005|        129.759995|        132.330002|        131.750458|         31152700|\n",
      "+-------+----------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Describe data\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+------+--------+\n",
      "|summary|  Open|  High|   Low| Close|  Volume|\n",
      "+-------+------+------+------+------+--------+\n",
      "|  count|252.00|252.00|252.00|252.00|     252|\n",
      "|   mean|118.09|119.29|116.92|118.13| 7487978|\n",
      "| stddev|  5.06|  5.01|  5.03|  4.97| 4370560|\n",
      "|    min|105.20|106.84|102.00|104.05| 2227400|\n",
      "|    max|132.39|133.38|129.76|132.33|31152700|\n",
      "+-------+------+------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(result['summary'],\n",
    "             format_number(result['Open'].cast('float'),2).alias('Open'),\n",
    "             format_number(result['High'].cast('float'),2).alias('High'),\n",
    "             format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "             format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "             result['Volume'].cast('int').alias('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|3.066011193332087E-5|\n",
      "|3.439330603705917E-5|\n",
      "|3.310876612579554E-5|\n",
      "|4.283801301841175E-5|\n",
      "|3.559290467625899...|\n",
      "|3.037431130311092E-5|\n",
      "|2.531382054063627...|\n",
      "|1.967258645858677...|\n",
      "|2.700069446771838...|\n",
      "|2.918083837465922...|\n",
      "|2.396701120744343...|\n",
      "|3.015534164308973...|\n",
      "|3.703215669478123...|\n",
      "|1.808639028866743E-5|\n",
      "|1.504627739771965...|\n",
      "|1.947071596513868...|\n",
      "|1.426684280052840...|\n",
      "|1.536970469186477...|\n",
      "|1.492615222916809...|\n",
      "|2.488112564008778...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a new dataframe\n",
    "df2 = df.withColumn(\"HV Ratio\",df['High']/df['Volume'])\n",
    "df2.select('HV Ratio').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-04-20'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what day have the Peak High in Price\n",
    "df.orderBy(df['High'].desc()).head(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(Close)|\n",
      "+------------------+\n",
      "|118.12956356349204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the mean of the Close column\n",
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"Close\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   31152700|    2227400|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the max and min of the Volume column\n",
    "from pyspark.sql.functions import max,min\n",
    "df.select(max(\"Volume\"),min('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many days was the Close higher than 60$\n",
    "df.filter('Close>100').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.095238095238095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What percentage of the time was High lower than 120$?\n",
    "#In other words, (Number of Days High>80)/(Total Days in dataset)\n",
    "(df.filter(df['High']<120).count()/df.count())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|corr(High, Volume)|\n",
      "+------------------+\n",
      "|0.2719334383137132|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#what is Pearson correlation b.w High and Volume\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('High','Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the max High per year?\n",
    "from pyspark.sql.functions import year\n",
    "yeardf = df.withColumn(\"Year\",year(df['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = yeardf.groupBy('Year').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Year| max(High)|\n",
      "+----+----------+\n",
      "|2019|125.379997|\n",
      "|2020|133.380005|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df.select('Year','max(High)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is avearge Close for each Calendar Month\n",
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthdf = df.withColumn('Month',month('Date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthavgs = monthdf.select(['Month','Close']).groupBy('Month').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|        avg(Close)|\n",
      "+-----+------------------+\n",
      "|    1|116.21095204761906|\n",
      "|    2|115.63631521052632|\n",
      "|    3|114.41727345454545|\n",
      "|    4|125.78809571428572|\n",
      "|    5|124.07350000000001|\n",
      "|    6|120.50636331818183|\n",
      "|    7|116.76333247619047|\n",
      "|    8|110.29409072727275|\n",
      "|    9|116.83600119999998|\n",
      "|   10|118.59434769565217|\n",
      "|   11|119.31650085000001|\n",
      "|   12|119.55095242857145|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthavgs.select('Month','avg(Close)').orderBy('Month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
